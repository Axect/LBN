{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef3b0b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Uniform\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau, ExponentialLR, OneCycleLR\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "# from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "PATH_DATASETS = \".\"\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "BATCH_SIZE = 512 if AVAIL_GPUS else 64\n",
    "# BATCH_SIZE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "344551a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79c02cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aabe3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mass(x):\n",
    "    return torch.sqrt(x[...,0]**2 - x[...,1]**2 - x[...,2]**2 - x[...,3]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c03b703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyData(Dataset):\n",
    "    def __init__(self, p, g):\n",
    "        self.p = p\n",
    "        self.g = g\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.p.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.p[idx,:], self.g[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9de29ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_four_vectors(n, p_low=-100., p_high=100., m_low=0.1, m_high=50.):\n",
    "    \"\"\"\n",
    "    Creates a numpy array with shape ``n + (4,)`` describing four-vectors of particles whose\n",
    "    momentum components are uniformly distributed between *p_low* and *p_high*, and masses between\n",
    "    *m_low* and *m_high*.\n",
    "    \"\"\"\n",
    "    # create random four-vectors\n",
    "    if not isinstance(n, tuple):\n",
    "        n = (n,)\n",
    "    vecs = np.random.uniform(p_low, p_high, n + (4,)).astype(np.float32)\n",
    "\n",
    "    # the energy is also random and might be lower than the momentum,\n",
    "    # so draw uniformly distributed masses, and compute and insert the energy\n",
    "    m = np.abs(np.random.uniform(m_low, m_high, n))\n",
    "    p = np.sqrt(np.sum(vecs[..., 1:]**2, axis=-1))\n",
    "    E = (p**2 + m**2)**0.5\n",
    "    vecs[..., 0] = E\n",
    "\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbe21bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_mass(p):\n",
    "    m = mass(p)\n",
    "    g1 = torch.where(m >= 25.)[0]\n",
    "    g = torch.zeros(m.shape)\n",
    "    g[g1] = 1.0\n",
    "    return g.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74d33198",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = create_four_vectors(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9a5d998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e35af7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.3323, 26.3183, 14.2749, 32.9568, 49.4772, 32.2481, 15.2248,  7.1555,\n",
       "        24.4562,  8.4541])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mass(torch.tensor(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68d5a6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_by_mass(torch.tensor(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4751ca",
   "metadata": {},
   "source": [
    "## 1. DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81d107a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(pl.LightningModule):\n",
    "    def __init__(self, N=20000, hparams=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.N = N\n",
    "        \n",
    "        hidden_layer = hparams[\"hidden_layer\"]\n",
    "        hidden_depth = hparams[\"hidden_depth\"]\n",
    "        learning_rate = hparams[\"learning_rate\"]\n",
    "        batch_size = hparams[\"batch_size\"]\n",
    "        \n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.hidden_depth = hidden_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "#         self.max_lr = hparams[\"max_lr\"]\n",
    "        self.epochs = hparams[\"epochs\"]\n",
    "        \n",
    "        layers = [nn.Linear(4, hidden_layer), nn.ReLU(), nn.BatchNorm1d(hidden_layer)]\n",
    "        for i in range(hidden_depth):\n",
    "            layers.extend([\n",
    "                nn.Linear(hidden_layer, hidden_layer),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_layer)\n",
    "            ])\n",
    "        layers.append(nn.Linear(hidden_layer, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "        hparams = {\n",
    "            \"hidden_layer\": hidden_layer,\n",
    "            \"hidden_depth\": hidden_depth,\n",
    "            \"batch_size\": batch_size,\n",
    "        }\n",
    "    \n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.accuracy = torchmetrics.Accuracy()\n",
    "        self.ds = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        p, g = batch\n",
    "        m = self(p)\n",
    "        loss = F.binary_cross_entropy_with_logits(m, g)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        p, g = batch\n",
    "        m = self(p)\n",
    "        loss = F.binary_cross_entropy_with_logits(m, g)\n",
    "        acc = self.accuracy(\n",
    "            torch.round(torch.sigmoid(m)),\n",
    "            g.int()\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            criterion = nn.MSELoss(reduction=\"mean\")\n",
    "            rmse = torch.sqrt(criterion(m+25., mass(p)) + 1e-6)\n",
    "        \n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', acc)\n",
    "        self.log('rmse', rmse)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        p = torch.tensor(create_four_vectors(self.N))\n",
    "        g = group_by_mass(p)\n",
    "        self.ds = ToyData(p, g)\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        N_train = self.N // 10 * 7\n",
    "        N_val = self.N - N_train\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.ds_train, self.ds_val = random_split(self.ds, [N_train, N_val])\n",
    "        if stage == \"test\" or stage is None:\n",
    "            _, self.ds_test = random_split(self.ds, [N_train, N_val])\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.ds_train, batch_size=self.batch_size)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.ds_val, batch_size=self.batch_size)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.ds_test, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "668f4d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "hparams = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"epochs\": 200,\n",
    "    \"hidden_layer\": 64,\n",
    "    \"hidden_depth\": 3,\n",
    "}\n",
    "\n",
    "model = DNN(\n",
    "    hparams=hparams\n",
    ")\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project='LBN_Tutorial'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    logger=wandb_logger,\n",
    "    max_epochs=hparams[\"epochs\"],\n",
    "    gpus=AVAIL_GPUS,\n",
    "    enable_progress_bar=False,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=20, mode=\"min\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcbf73c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maxect\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/axect/LBN_Tutorial/runs/13f3zs6m\" target=\"_blank\">confused-field-13</a></strong> to <a href=\"https://wandb.ai/axect/LBN_Tutorial\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name     | Type       | Params\n",
      "----------------------------------------\n",
      "0 | net      | Sequential | 13.4 K\n",
      "1 | accuracy | Accuracy   | 0     \n",
      "----------------------------------------\n",
      "13.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "13.4 K    Total params\n",
      "0.054     Total estimated model params size (MB)\n",
      "/home/xteca/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/xteca/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([512])) that is different to the input size (torch.Size([512, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Global seed set to 125\n",
      "/home/xteca/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/xteca/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:428: UserWarning: The number of training samples (28) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "/home/xteca/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([368])) that is different to the input size (torch.Size([368, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55a2568f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 29751... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>rmse</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_acc</td><td>▁▄▆▇▇███████████████████████████████████</td></tr><tr><td>val_loss</td><td>█▇▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>154</td></tr><tr><td>rmse</td><td>15.92025</td></tr><tr><td>trainer/global_step</td><td>4339</td></tr><tr><td>val_acc</td><td>0.94483</td></tr><tr><td>val_loss</td><td>0.13412</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">confused-field-13</strong>: <a href=\"https://wandb.ai/axect/LBN_Tutorial/runs/13f3zs6m\" target=\"_blank\">https://wandb.ai/axect/LBN_Tutorial/runs/13f3zs6m</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220325_123346-13f3zs6m/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91ba5390",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_data = list(iter(model.val_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccef85c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = val_data[0][0]\n",
    "gs = val_data[0][1]\n",
    "ms = mass(ps)\n",
    "m_hats = model(ps)\n",
    "g_hats = torch.round(torch.sigmoid(m_hats))\n",
    "m_hats = m_hats + 25\n",
    "\n",
    "ps = ps.detach().numpy()\n",
    "gs = gs.detach().numpy()\n",
    "ms = ms.detach().numpy()\n",
    "m_hats = m_hats.detach().numpy()\n",
    "g_hats = g_hats.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46e8ca85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m</th>\n",
       "      <th>m_hat</th>\n",
       "      <th>g</th>\n",
       "      <th>g_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.885422</td>\n",
       "      <td>23.889009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46.144024</td>\n",
       "      <td>29.385429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.520767</td>\n",
       "      <td>32.969849</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.627579</td>\n",
       "      <td>21.460138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.446548</td>\n",
       "      <td>22.690598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>23.617651</td>\n",
       "      <td>25.929300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>27.212305</td>\n",
       "      <td>28.927141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>2.279216</td>\n",
       "      <td>18.501684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>40.928410</td>\n",
       "      <td>31.580730</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>24.121185</td>\n",
       "      <td>26.227776</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>512 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             m      m_hat    g  g_hat\n",
       "0    21.885422  23.889009  0.0    0.0\n",
       "1    46.144024  29.385429  1.0    1.0\n",
       "2    41.520767  32.969849  1.0    1.0\n",
       "3     6.627579  21.460138  0.0    0.0\n",
       "4    17.446548  22.690598  0.0    0.0\n",
       "..         ...        ...  ...    ...\n",
       "507  23.617651  25.929300  0.0    1.0\n",
       "508  27.212305  28.927141  1.0    1.0\n",
       "509   2.279216  18.501684  0.0    0.0\n",
       "510  40.928410  31.580730  1.0    1.0\n",
       "511  24.121185  26.227776  0.0    1.0\n",
       "\n",
       "[512 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg = pd.DataFrame({\n",
    "    \"m\": ms,\n",
    "    \"m_hat\": m_hats[:,0],\n",
    "    \"g\": gs[:,0],\n",
    "    \"g_hat\": g_hats[:,0]\n",
    "})\n",
    "\n",
    "dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4610dd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37039825320243835"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.sum((ms - m_hats[:,0])**2)) / len(ms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
